{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesesary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"NVIDIA_API_KEY\"]=os.getenv('NVIDIA_API_KEY')\n",
    "os.environ['LANGSMITH_API_KEY']=os.getenv('LANGSMITH_API_KEY')\n",
    "os.environ['LANGSMITH_TRACING']=os.getenv('LANGSMITH_TRACING')\n",
    "os.environ['LANGSMITH_PROJECT']=os.getenv('LANGSMITH_PROJECT',\"Your-proejct-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>page</td>\n",
       "      <td>https://python.langchain.com/api_reference/mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>page</td>\n",
       "      <td>https://python.langchain.com/v0.2/api_referenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>page</td>\n",
       "      <td>https://python.langchain.com/docs/contributing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>page</td>\n",
       "      <td>https://api.python.langchain.com/en/latest/cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>page</td>\n",
       "      <td>https://python.langchain.com/api_reference/mon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                                URL\n",
       "0  page  https://python.langchain.com/api_reference/mon...\n",
       "1  page  https://python.langchain.com/v0.2/api_referenc...\n",
       "2  page  https://python.langchain.com/docs/contributing...\n",
       "3  page  https://api.python.langchain.com/en/latest/cha...\n",
       "4  page  https://python.langchain.com/api_reference/mon..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../langchain_crawl_results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_urls = (df[df['Type']=='page'].URL).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "loader = UnstructuredURLLoader(list_of_urls[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(list_of_urls[:10])\n",
    "\n",
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\\n\\nSource code for langchain_core.utils.json\\n\\nfrom __future__ import annotations\\n\\nimport json\\nimport re\\nfrom typing import Any, Callable, List\\n\\nfrom langchain_core.exceptions import OutputParserException\\n\\n\\ndef _replace_new_line(match: re.Match[str]) -> str:\\n    value = match.group(2)\\n    value = re.sub(r\"\\\\n\", r\"\\\\\\\\n\", value)\\n    value = re.sub(r\"\\\\r\", r\"\\\\\\\\r\", value)\\n    value = re.sub(r\"\\\\t\", r\"\\\\\\\\t\", value)\\n    value = re.sub(r\\'(?<!\\\\\\\\)\"\\', r\"\\\\\"\", value)\\n\\n    return match.group(1) + value + match.group(3)\\n\\n\\ndef _custom_parser(multiline_string: str) -> str:\\n    \"\"\"\\n    The LLM response for `action_input` may be a multiline\\n    string containing unescaped newlines, tabs or quotes. This function\\n    replaces those characters with their escaped counterparts.\\n    (newlines in JSON must be double-escaped: `\\\\\\\\n`)\\n    \"\"\"\\n    if isinstance(multiline_string, (bytes, bytearray)):\\n        multiline_string = multiline_string.decode()\\n\\n    multiline_string = re.sub(\\n        r\\'(\"action_input\"\\\\:\\\\s*\")(.*?)(\")\\',\\n        _replace_new_line,\\n        multiline_string,\\n        flags=re.DOTALL,\\n    )\\n\\n    return multiline_string\\n\\n\\n# Adapted from https://github.com/KillianLucas/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py\\n# MIT License\\n\\n\\n\\n[docs]def parse_partial_json(s: str, *, strict: bool = False) -> Any: \"\"\"Parse a JSON string that may be missing closing braces. Args: s: The JSON string to parse. strict: Whether to use strict parsing. Defaults to False. Returns: The parsed JSON object as a Python dictionary. \"\"\" # Attempt to parse the string as-is. try: return json.loads(s, strict=strict) except json.JSONDecodeError: pass # Initialize variables. new_chars = [] stack = [] is_inside_string = False escaped = False # Process each character in the string one at a time. for char in s: if is_inside_string: if char == \\'\"\\' and not escaped: is_inside_string = False elif char == \"\\\\n\" and not escaped: char = \"\\\\\\\\n\" # Replace the newline character with the escape sequence. elif char == \"\\\\\\\\\": escaped = not escaped else: escaped = False else: if char == \\'\"\\': is_inside_string = True escaped = False elif char == \"{\": stack.append(\"}\") elif char == \"[\": stack.append(\"]\") elif char == \"}\" or char == \"]\": if stack and stack[-1] == char: stack.pop() else: # Mismatched closing character; the input is malformed. return None # Append the processed character to the new string. new_chars.append(char) # If we\\'re still inside a string at the end of processing, # we need to close the string. if is_inside_string: new_chars.append(\\'\"\\') # Reverse the stack to get the closing characters. stack.reverse() # Try to parse mods of string until we succeed or run out of characters. while new_chars: # Close any remaining open structures in the reverse # order that they were opened. # Attempt to parse the modified string as JSON. try: return json.loads(\"\".join(new_chars + stack), strict=strict) except json.JSONDecodeError: # If we still can\\'t parse the string as JSON, # try removing the last character new_chars.pop() # If we got here, we ran out of characters to remove # and still couldn\\'t parse the string as JSON, so return the parse error # for the original string. return json.loads(s, strict=strict)\\n\\n\\n\\n_json_markdown_re = re.compile(r\"```(json)?(.*)\", re.DOTALL)\\n\\n\\n\\n[docs]def parse_json_markdown( json_string: str, *, parser: Callable[[str], Any] = parse_partial_json ) -> dict: \"\"\"Parse a JSON string from a Markdown string. Args: json_string: The Markdown string. Returns: The parsed JSON object as a Python dictionary. \"\"\" try: return _parse_json(json_string, parser=parser) except json.JSONDecodeError: # Try to find JSON string within triple backticks match = _json_markdown_re.search(json_string) # If no match found, assume the entire string is a JSON string if match is None: json_str = json_string else: # If match found, use the content within the backticks json_str = match.group(2) return _parse_json(json_str, parser=parser)\\n\\n\\n\\n_json_strip_chars = \" \\\\n\\\\r\\\\t`\"\\n\\n\\ndef _parse_json(\\n    json_str: str, *, parser: Callable[[str], Any] = parse_partial_json\\n) -> dict:\\n    # Strip whitespace,newlines,backtick from the start and end\\n    json_str = json_str.strip(_json_strip_chars)\\n\\n    # handle newlines and other special characters inside the returned value\\n    json_str = _custom_parser(json_str)\\n\\n    # Parse the JSON string into a Python dictionary\\n    return parser(json_str)\\n\\n\\n\\n[docs]def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict: \"\"\" Parse a JSON string from a Markdown string and check that it contains the expected keys. Args: text: The Markdown string. expected_keys: The expected keys in the JSON string. Returns: The parsed JSON object as a Python dictionary. Raises: OutputParserException: If the JSON string is invalid or does not contain the expected keys. \"\"\" try: json_obj = parse_json_markdown(text) except json.JSONDecodeError as e: raise OutputParserException(f\"Got invalid JSON object. Error: {e}\") from e for key in expected_keys: if key not in json_obj: raise OutputParserException( f\"Got invalid return object. Expected key `{key}` \" f\"to be present, but got {json_obj}\" ) return json_obj\\n\\nÂ© 2023, LangChain, Inc. . Last updated on Dec 09, 2024.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/api_reference/mongodb/retrievers/pipelines/pipelines/retrievers/langchain_mongodb.retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='Source code for langchain_community.llms.databricks\\n\\nimport os\\nimport re\\nimport warnings\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional\\n\\nimport requests\\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\\nfrom langchain_core.language_models import LLM\\nfrom langchain_core.pydantic_v1 import (\\n    BaseModel,\\n    Field,\\n    PrivateAttr,\\n    root_validator,\\n    validator,\\n)\\n\\n__all__ = [\"Databricks\"]\\n\\n\\nclass _DatabricksClientBase(BaseModel, ABC):\\n    \"\"\"A base JSON API client that talks to Databricks.\"\"\"\\n\\n    api_url: str\\n    api_token: str\\n\\n    def request(self, method: str, url: str, request: Any) -> Any:\\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\\n        response = requests.request(\\n            method=method, url=url, headers=headers, json=request\\n        )\\n        # TODO: error handling and automatic retries\\n        if not response.ok:\\n            raise ValueError(f\"HTTP {response.status_code} error: {response.text}\")\\n        return response.json()\\n\\n    def _get(self, url: str) -> Any:\\n        return self.request(\"GET\", url, None)\\n\\n    def _post(self, url: str, request: Any) -> Any:\\n        return self.request(\"POST\", url, request)\\n\\n    @abstractmethod\\n    def post(\\n        self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None\\n    ) -> Any: ...\\n\\n    @property\\n    def llm(self) -> bool:\\n        return False\\n\\n\\ndef _transform_completions(response: Dict[str, Any]) -> str:\\n    return response[\"choices\"][0][\"text\"]\\n\\n\\ndef _transform_llama2_chat(response: Dict[str, Any]) -> str:\\n    return response[\"candidates\"][0][\"text\"]\\n\\n\\ndef _transform_chat(response: Dict[str, Any]) -> str:\\n    return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass _DatabricksServingEndpointClient(_DatabricksClientBase):\\n    \"\"\"An API client that talks to a Databricks serving endpoint.\"\"\"\\n\\n    host: str\\n    endpoint_name: str\\n    databricks_uri: str\\n    client: Any = None\\n    external_or_foundation: bool = False\\n    task: Optional[str] = None\\n\\n    def __init__(self, **data: Any):\\n        super().__init__(**data)\\n\\n        try:\\n            from mlflow.deployments import get_deploy_client\\n\\n            self.client = get_deploy_client(self.databricks_uri)\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Failed to create the client. \"\\n                \"Please install mlflow with `pip install mlflow`.\"\\n            ) from e\\n\\n        endpoint = self.client.get_endpoint(self.endpoint_name)\\n        self.external_or_foundation = endpoint.get(\"endpoint_type\", \"\").lower() in (\\n            \"external_model\",\\n            \"foundation_model_api\",\\n        )\\n        if self.task is None:\\n            self.task = endpoint.get(\"task\")\\n\\n    @property\\n    def llm(self) -> bool:\\n        return self.task in (\"llm/v1/chat\", \"llm/v1/completions\", \"llama2/chat\")\\n\\n    @root_validator(pre=True)\\n    def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if \"api_url\" not in values:\\n            host = values[\"host\"]\\n            endpoint_name = values[\"endpoint_name\"]\\n            api_url = f\"https://{host}/serving-endpoints/{endpoint_name}/invocations\"\\n            values[\"api_url\"] = api_url\\n        return values\\n\\n    def post(\\n        self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None\\n    ) -> Any:\\n        if self.external_or_foundation:\\n            resp = self.client.predict(endpoint=self.endpoint_name, inputs=request)\\n            if transform_output_fn:\\n                return transform_output_fn(resp)\\n\\n            if self.task == \"llm/v1/chat\":\\n                return _transform_chat(resp)\\n            elif self.task == \"llm/v1/completions\":\\n                return _transform_completions(resp)\\n\\n            return resp\\n        else:\\n            # See https://docs.databricks.com/machine-learning/model-serving/score-model-serving-endpoints.html\\n            wrapped_request = {\"dataframe_records\": [request]}\\n            response = self.client.predict(\\n                endpoint=self.endpoint_name, inputs=wrapped_request\\n            )\\n            preds = response[\"predictions\"]\\n            # For a single-record query, the result is not a list.\\n            pred = preds[0] if isinstance(preds, list) else preds\\n            if self.task == \"llama2/chat\":\\n                return _transform_llama2_chat(pred)\\n            return transform_output_fn(pred) if transform_output_fn else pred\\n\\n\\nclass _DatabricksClusterDriverProxyClient(_DatabricksClientBase):\\n    \"\"\"An API client that talks to a Databricks cluster driver proxy app.\"\"\"\\n\\n    host: str\\n    cluster_id: str\\n    cluster_driver_port: str'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='class _DatabricksClusterDriverProxyClient(_DatabricksClientBase):\\n    \"\"\"An API client that talks to a Databricks cluster driver proxy app.\"\"\"\\n\\n    host: str\\n    cluster_id: str\\n    cluster_driver_port: str\\n\\n    @root_validator(pre=True)\\n    def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if \"api_url\" not in values:\\n            host = values[\"host\"]\\n            cluster_id = values[\"cluster_id\"]\\n            port = values[\"cluster_driver_port\"]\\n            api_url = f\"https://{host}/driver-proxy-api/o/0/{cluster_id}/{port}\"\\n            values[\"api_url\"] = api_url\\n        return values\\n\\n    def post(\\n        self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None\\n    ) -> Any:\\n        resp = self._post(self.api_url, request)\\n        return transform_output_fn(resp) if transform_output_fn else resp\\n\\n\\n\\n[docs]def get_repl_context() -> Any: \"\"\"Get the notebook REPL context if running inside a Databricks notebook. Returns None otherwise. \"\"\" try: from dbruntime.databricks_repl_context import get_context return get_context() except ImportError: raise ImportError( \"Cannot access dbruntime, not running inside a Databricks notebook.\" )\\n\\n[docs]def get_default_host() -> str: \"\"\"Get the default Databricks workspace hostname. Raises an error if the hostname cannot be automatically determined. \"\"\" host = os.getenv(\"DATABRICKS_HOST\") if not host: try: host = get_repl_context().browserHostName if not host: raise ValueError(\"context doesn\\'t contain browserHostName.\") except Exception as e: raise ValueError( \"host was not set and cannot be automatically inferred. Set \" f\"environment variable \\'DATABRICKS_HOST\\'. Received error: {e}\" ) # TODO: support Databricks CLI profile host = host.lstrip(\"https://\").lstrip(\"http://\").rstrip(\"/\") return host\\n\\n[docs]def get_default_api_token() -> str: \"\"\"Get the default Databricks personal access token. Raises an error if the token cannot be automatically determined. \"\"\" if api_token := os.getenv(\"DATABRICKS_TOKEN\"): return api_token try: api_token = get_repl_context().apiToken if not api_token: raise ValueError(\"context doesn\\'t contain apiToken.\") except Exception as e: raise ValueError( \"api_token was not set and cannot be automatically inferred. Set \" f\"environment variable \\'DATABRICKS_TOKEN\\'. Received error: {e}\" ) # TODO: support Databricks CLI profile return api_token\\n\\n\\n\\ndef _is_hex_string(data: str) -> bool:\\n    \"\"\"Checks if a data is a valid hexadecimal string using a regular expression.\"\"\"\\n    if not isinstance(data, str):\\n        return False\\n    pattern = r\"^[0-9a-fA-F]+$\"\\n    return bool(re.match(pattern, data))\\n\\n\\ndef _load_pickled_fn_from_hex_string(\\n    data: str, allow_dangerous_deserialization: Optional[bool]\\n) -> Callable:\\n    \"\"\"Loads a pickled function from a hexadecimal string.\"\"\"\\n    if not allow_dangerous_deserialization:\\n        raise ValueError(\\n            \"This code relies on the pickle module. \"\\n            \"You will need to set allow_dangerous_deserialization=True \"\\n            \"if you want to opt-in to allow deserialization of data using pickle.\"\\n            \"Data can be compromised by a malicious actor if \"\\n            \"not handled properly to include \"\\n            \"a malicious payload that when deserialized with \"\\n            \"pickle can execute arbitrary code on your machine.\"\\n        )\\n\\n    try:\\n        import cloudpickle\\n    except Exception as e:\\n        raise ValueError(f\"Please install cloudpickle>=2.0.0. Error: {e}\")\\n\\n    try:\\n        return cloudpickle.loads(bytes.fromhex(data))  # ignore[pickle]: explicit-opt-in\\n    except Exception as e:\\n        raise ValueError(\\n            f\"Failed to load the pickled function from a hexadecimal string. Error: {e}\"\\n        )\\n\\n\\ndef _pickle_fn_to_hex_string(fn: Callable) -> str:\\n    \"\"\"Pickles a function and returns the hexadecimal string.\"\"\"\\n    try:\\n        import cloudpickle\\n    except Exception as e:\\n        raise ValueError(f\"Please install cloudpickle>=2.0.0. Error: {e}\")\\n\\n    try:\\n        return cloudpickle.dumps(fn).hex()\\n    except Exception as e:\\n        raise ValueError(f\"Failed to pickle the function: {e}\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='[docs]class Databricks(LLM): \"\"\"Databricks serving endpoint or a cluster driver proxy app for LLM. It supports two endpoint types: * **Serving endpoint** (recommended for both production and development). We assume that an LLM was deployed to a serving endpoint. To wrap it as an LLM you must have \"Can Query\" permission to the endpoint. Set ``endpoint_name`` accordingly and do not set ``cluster_id`` and ``cluster_driver_port``. If the underlying model is a model registered by MLflow, the expected model signature is: * inputs:: [{\"name\": \"prompt\", \"type\": \"string\"}, {\"name\": \"stop\", \"type\": \"list[string]\"}] * outputs: ``[{\"type\": \"string\"}]`` If the underlying model is an external or foundation model, the response from the endpoint is automatically transformed to the expected format unless ``transform_output_fn`` is provided. * **Cluster driver proxy app** (recommended for interactive development). One can load an LLM on a Databricks interactive cluster and start a local HTTP server on the driver node to serve the model at ``/`` using HTTP POST method with JSON input/output. Please use a port number between ``[3000, 8000]`` and let the server listen to the driver IP address or simply ``0.0.0.0`` instead of localhost only. To wrap it as an LLM you must have \"Can Attach To\" permission to the cluster. Set ``cluster_id`` and ``cluster_driver_port`` and do not set ``endpoint_name``. The expected server schema (using JSON schema) is: * inputs:: {\"type\": \"object\", \"properties\": { \"prompt\": {\"type\": \"string\"}, \"stop\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"prompt\"]}` * outputs: ``{\"type\": \"string\"}`` If the endpoint model signature is different or you want to set extra params, you can use `transform_input_fn` and `transform_output_fn` to apply necessary transformations before and after the query. \"\"\" host: str = Field(default_factory=get_default_host) \"\"\"Databricks workspace hostname. If not provided, the default value is determined by * the ``DATABRICKS_HOST`` environment variable if present, or * the hostname of the current Databricks workspace if running inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode. \"\"\" api_token: str = Field(default_factory=get_default_api_token) \"\"\"Databricks personal access token. If not provided, the default value is determined by * the ``DATABRICKS_TOKEN`` environment variable if present, or * an automatically generated temporary token if running inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode. \"\"\" endpoint_name: Optional[str] = None \"\"\"Name of the model serving endpoint. You must specify the endpoint name to connect to a model serving endpoint. You must not set both ``endpoint_name`` and ``cluster_id``. \"\"\" cluster_id: Optional[str] = None \"\"\"ID of the cluster if connecting to a cluster driver proxy app. If neither ``endpoint_name`` nor ``cluster_id`` is not provided and the code runs inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode, the current cluster ID is used as default. You must not set both ``endpoint_name`` and ``cluster_id``. \"\"\" cluster_driver_port: Optional[str] = None \"\"\"The port number used by the HTTP server running on the cluster driver node. The server should listen on the driver IP address or simply ``0.0.0.0`` to connect. We recommend the server using a port number between ``[3000, 8000]``. \"\"\" model_kwargs: Optional[Dict[str, Any]] = None \"\"\" Deprecated. Please use ``extra_params`` instead. Extra parameters to pass to the endpoint. \"\"\" transform_input_fn: Optional[Callable] = None \"\"\"A function that transforms ``{prompt, stop, **kwargs}`` into a JSON-compatible request object that the endpoint accepts. For example, you can apply a prompt template to the input prompt. \"\"\" transform_output_fn: Optional[Callable[..., str]] = None \"\"\"A function that transforms the output from the endpoint to the generated text. \"\"\" databricks_uri: str = \"databricks\" \"\"\"The databricks URI. Only used when using a serving endpoint.\"\"\" temperature: float = 0.0 \"\"\"The sampling temperature.\"\"\" n: int = 1 \"\"\"The number of completion choices to generate.\"\"\" stop: Optional[List[str]] = None \"\"\"The stop sequence.\"\"\" max_tokens: Optional[int] = None \"\"\"The maximum number of tokens to generate.\"\"\" extra_params: Dict[str, Any] = Field(default_factory=dict) \"\"\"Any extra parameters to pass to the endpoint.\"\"\" task: Optional[str] = None \"\"\"The task of the endpoint. Only used when using a serving endpoint. If not provided, the task is automatically inferred from the endpoint. \"\"\" allow_dangerous_deserialization: bool = False \"\"\"Whether to allow dangerous deserialization of the data which involves loading data using pickle. If the data has been modified by a malicious actor, it can deliver a malicious payload that results in execution of arbitrary code on the target machine. \"\"\" _client:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='endpoint.\"\"\" task: Optional[str] = None \"\"\"The task of the endpoint. Only used when using a serving endpoint. If not provided, the task is automatically inferred from the endpoint. \"\"\" allow_dangerous_deserialization: bool = False \"\"\"Whether to allow dangerous deserialization of the data which involves loading data using pickle. If the data has been modified by a malicious actor, it can deliver a malicious payload that results in execution of arbitrary code on the target machine. \"\"\" _client: _DatabricksClientBase = PrivateAttr() class Config: extra = \"forbid\" underscore_attrs_are_private = True @property def _llm_params(self) -> Dict[str, Any]: params: Dict[str, Any] = { \"temperature\": self.temperature, \"n\": self.n, } if self.stop: params[\"stop\"] = self.stop if self.max_tokens is not None: params[\"max_tokens\"] = self.max_tokens return params @validator(\"cluster_id\", always=True) def set_cluster_id(cls, v: Any, values: Dict[str, Any]) -> Optional[str]: if v and values[\"endpoint_name\"]: raise ValueError(\"Cannot set both endpoint_name and cluster_id.\") elif values[\"endpoint_name\"]: return None elif v: return v else: try: if v := get_repl_context().clusterId: return v raise ValueError(\"Context doesn\\'t contain clusterId.\") except Exception as e: raise ValueError( \"Neither endpoint_name nor cluster_id was set. \" \"And the cluster_id cannot be automatically determined. Received\" f\" error: {e}\" ) @validator(\"cluster_driver_port\", always=True) def set_cluster_driver_port(cls, v: Any, values: Dict[str, Any]) -> Optional[str]: if v and values[\"endpoint_name\"]: raise ValueError(\"Cannot set both endpoint_name and cluster_driver_port.\") elif values[\"endpoint_name\"]: return None elif v is None: raise ValueError( \"Must set cluster_driver_port to connect to a cluster driver.\" ) elif int(v) <= 0: raise ValueError(f\"Invalid cluster_driver_port: {v}\") else: return v @validator(\"model_kwargs\", always=True) def set_model_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]: if v: assert \"prompt\" not in v, \"model_kwargs must not contain key \\'prompt\\'\" assert \"stop\" not in v, \"model_kwargs must not contain key \\'stop\\'\" return v def __init__(self, **data: Any): if \"transform_input_fn\" in data and _is_hex_string(data[\"transform_input_fn\"]): data[\"transform_input_fn\"] = _load_pickled_fn_from_hex_string( data=data[\"transform_input_fn\"], allow_dangerous_deserialization=data.get( \"allow_dangerous_deserialization\" ), ) if \"transform_output_fn\" in data and _is_hex_string( data[\"transform_output_fn\"] ): data[\"transform_output_fn\"] = _load_pickled_fn_from_hex_string( data=data[\"transform_output_fn\"], allow_dangerous_deserialization=data.get( \"allow_dangerous_deserialization\" ), ) super().__init__(**data) if self.model_kwargs is not None and self.extra_params is not None: raise ValueError(\"Cannot set both extra_params and extra_params.\") elif self.model_kwargs is not None: warnings.warn( \"model_kwargs is deprecated. Please use extra_params instead.\", DeprecationWarning, ) if self.endpoint_name: self._client = _DatabricksServingEndpointClient( host=self.host, api_token=self.api_token, endpoint_name=self.endpoint_name, databricks_uri=self.databricks_uri, task=self.task, ) elif self.cluster_id and self.cluster_driver_port: self._client = _DatabricksClusterDriverProxyClient( # type: ignore[call-arg] host=self.host, api_token=self.api_token, cluster_id=self.cluster_id, cluster_driver_port=self.cluster_driver_port, ) else: raise ValueError( \"Must specify either endpoint_name or cluster_id/cluster_driver_port.\" ) @property def _default_params(self) -> Dict[str, Any]: \"\"\"Return default params.\"\"\" return { \"host\": self.host, # \"api_token\": self.api_token, # Never save the token \"endpoint_name\": self.endpoint_name, \"cluster_id\": self.cluster_id, \"cluster_driver_port\": self.cluster_driver_port, \"databricks_uri\": self.databricks_uri, \"model_kwargs\": self.model_kwargs, \"temperature\": self.temperature, \"n\": self.n, \"stop\": self.stop, \"max_tokens\": self.max_tokens, \"extra_params\": self.extra_params, \"task\": self.task, \"transform_input_fn\": None if self.transform_input_fn is None else _pickle_fn_to_hex_string(self.transform_input_fn), \"transform_output_fn\": None if self.transform_output_fn is None else _pickle_fn_to_hex_string(self.transform_output_fn), } @property def _identifying_params(self) -> Mapping[str, Any]: return self._default_params @property def _llm_type(self) -> str: \"\"\"Return type of llm.\"\"\" return \"databricks\" def _call( self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> str: \"\"\"Queries the LLM endpoint with the given prompt and stop sequence.\"\"\" # TODO: support callbacks request: Dict[str, Any] = {\"prompt\": prompt} if self._client.llm: request.update(self._llm_params) request.update(self.model_kwargs or self.extra_params) request.update(kwargs) if stop: request[\"stop\"] = stop if self.transform_input_fn: request = self.transform_input_fn(**request)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -> str: \"\"\"Queries the LLM endpoint with the given prompt and stop sequence.\"\"\" # TODO: support callbacks request: Dict[str, Any] = {\"prompt\": prompt} if self._client.llm: request.update(self._llm_params) request.update(self.model_kwargs or self.extra_params) request.update(kwargs) if stop: request[\"stop\"] = stop if self.transform_input_fn: request = self.transform_input_fn(**request) return self._client.post(request, transform_output_fn=self.transform_output_fn)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/contributing/documentation/setup'}, page_content='Open on GitHub\\n\\nSetup\\n\\nLangChain documentation consists of two components:\\n\\nMain Documentation: Hosted at python.langchain.com, this comprehensive resource serves as the primary user-facing documentation. It covers a wide array of topics, including tutorials, use cases, integrations, and more, offering extensive guidance on building with LangChain. The content for this documentation lives in the /docs directory of the monorepo.\\n\\nIn-code Documentation: This is documentation of the codebase itself, which is also used to generate the externally facing API Reference. The content for the API reference is autogenerated by scanning the docstrings in the codebase. For this reason we ask that developers document their code well.\\n\\nThe API Reference is largely autogenerated by sphinx from the code and is hosted by Read the Docs.\\n\\nWe appreciate all contributions to the documentation, whether it be fixing a typo, adding a new tutorial or example and whether it be in the main documentation or the API Reference.\\n\\nSimilar to linting, we recognize documentation can be annoying. If you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\\n\\nð Main Documentation\\u200b\\n\\nThe content for the main documentation is located in the /docs directory of the monorepo.\\n\\nThe documentation is written using a combination of ipython notebooks (.ipynb files) and markdown (.mdx files). The notebooks are converted to markdown and then built using Docusaurus 2.\\n\\nFeel free to make contributions to the main documentation! ð¥°\\n\\nAfter modifying the documentation:\\n\\nRun the linting and formatting commands (see below) to ensure that the documentation is well-formatted and free of errors.\\n\\nOptionally build the documentation locally to verify that the changes look good.\\n\\nMake a pull request with the changes.\\n\\nYou can preview and verify that the changes are what you wanted by clicking the View deployment or Visit Preview buttons on the pull request Conversation page. This will take you to a preview of the documentation changes.\\n\\nâï¸ Linting and Building Documentation Locally\\u200b\\n\\nAfter writing up the documentation, you may want to lint and build the documentation locally to ensure that it looks good and is free of errors.\\n\\nIf you\\'re unable to build it locally that\\'s okay as well, as you will be able to see a preview of the documentation on the pull request page.\\n\\nBuilding\\u200b\\n\\nThe code that builds the documentation is located in the /docs directory of the monorepo.\\n\\nIn the following commands, the prefix api_ indicates that those are operations for the API Reference.\\n\\nBefore building the documentation, it is always a good idea to clean the build directory:\\n\\nmake docs_clean\\nmake api_docs_clean\\n\\nNext, you can build the documentation as outlined below:\\n\\nmake docs_build\\nmake api_docs_build\\n\\ntip\\n\\nThe make api_docs_build command takes a long time. If you\\'re making cosmetic changes to the API docs and want to see how they look, use:\\n\\nmake api_docs_quick_preview\\n\\nwhich will just build a small subset of the API reference.\\n\\nFinally, run the link checker to ensure all links are valid:\\n\\nmake docs_linkcheck\\nmake api_docs_linkcheck\\n\\nLinting and Formatting\\u200b\\n\\nThe Main Documentation is linted from the monorepo root. To lint the main documentation, run the following from there:\\n\\nmake lint\\n\\nIf you have formatting-related errors, you can fix them automatically with:\\n\\nmake format\\n\\nâ¨ï¸ In-code Documentation\\u200b\\n\\nThe in-code documentation is largely autogenerated by sphinx from the code and is hosted by Read the Docs.\\n\\nFor the API reference to be useful, the codebase must be well-documented. This means that all functions, classes, and methods should have a docstring that explains what they do, what the arguments are, and what the return value is. This is a good practice in general, but it is especially important for LangChain because the API reference is the primary resource for developers to understand how to use the codebase.\\n\\nWe generally follow the Google Python Style Guide for docstrings.\\n\\nHere is an example of a well-documented function:\\n\\ndef my_function(arg1: int, arg2: str) -> float:\\n    \"\"\"This is a short description of the function. (It should be a single sentence.)\\n\\n    This is a longer description of the function. It should explain what\\n    the function does, what the arguments are, and what the return value is.\\n    It should wrap at 88 characters.\\n\\n    Examples:\\n        This is a section for examples of how to use the function.\\n\\n        .. code-block:: python\\n\\n            my_function(1, \"hello\")\\n\\n    Args:\\n        arg1: This is a description of arg1. We do not need to specify the type since\\n            it is already specified in the function signature.\\n        arg2: This is a description of arg2.\\n\\n    Returns:\\n        This is a description of the return value.\\n    \"\"\"\\n    return 3.14\\n\\nLinting and Formatting\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/contributing/documentation/setup'}, page_content='Examples:\\n        This is a section for examples of how to use the function.\\n\\n        .. code-block:: python\\n\\n            my_function(1, \"hello\")\\n\\n    Args:\\n        arg1: This is a description of arg1. We do not need to specify the type since\\n            it is already specified in the function signature.\\n        arg2: This is a description of arg2.\\n\\n    Returns:\\n        This is a description of the return value.\\n    \"\"\"\\n    return 3.14\\n\\nLinting and Formatting\\u200b\\n\\nThe in-code documentation is linted from the directories belonging to the packages being documented.\\n\\nFor example, if you\\'re working on the langchain-community package, you would change the working directory to the langchain-community directory:\\n\\ncd [root]/libs/langchain-community\\n\\nThen you can run the following commands to lint and format the in-code documentation:\\n\\nmake format\\nmake lint\\n\\nVerify Documentation Changes\\u200b\\n\\nAfter pushing documentation changes to the repository, you can preview and verify that the changes are what you wanted by clicking the View deployment or Visit Preview buttons on the pull request Conversation page. This will take you to a preview of the documentation changes. This preview is created by Vercel.\\n\\nEdit this page\\n\\nWas this page helpful?\\n\\nð Main Documentation\\n\\nâï¸ Linting and Building Documentation Locally\\n\\nBuilding\\n\\nLinting and Formatting\\n\\nâ¨ï¸ In-code Documentation\\n\\nLinting and Formatting\\n\\nVerify Documentation Changes'),\n",
       " Document(metadata={'source': 'https://api.python.langchain.com/en/latest/chains/langchain_community.chains.graph_qa.kuzu.remove_prefix.html'}, page_content='This is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\\n\\nlangchain_community.chains.graph_qa.kuzu.remove_prefix\\n\\nremove_prefix()\\n\\nlangchain_community.chains.graph_qa.kuzu.remove_prefixÂ¶\\n\\nÂ© 2023, LangChain, Inc. . Last updated on Dec 09, 2024.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/mongodb/chat_message_histories/loaders/loaders/utils/langchain_mongodb.utils.cosine_similarity.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\"),\n",
       " Document(metadata={'source': 'https://api.python.langchain.com/en/latest/_modules/langchain_core/utils/json.html'}, page_content='This is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\\n\\nSource code for langchain_core.utils.json\\n\\nfrom __future__ import annotations\\n\\nimport json\\nimport re\\nfrom typing import Any, Callable, List\\n\\nfrom langchain_core.exceptions import OutputParserException\\n\\n\\ndef _replace_new_line(match: re.Match[str]) -> str:\\n    value = match.group(2)\\n    value = re.sub(r\"\\\\n\", r\"\\\\\\\\n\", value)\\n    value = re.sub(r\"\\\\r\", r\"\\\\\\\\r\", value)\\n    value = re.sub(r\"\\\\t\", r\"\\\\\\\\t\", value)\\n    value = re.sub(r\\'(?<!\\\\\\\\)\"\\', r\"\\\\\"\", value)\\n\\n    return match.group(1) + value + match.group(3)\\n\\n\\ndef _custom_parser(multiline_string: str) -> str:\\n    \"\"\"\\n    The LLM response for `action_input` may be a multiline\\n    string containing unescaped newlines, tabs or quotes. This function\\n    replaces those characters with their escaped counterparts.\\n    (newlines in JSON must be double-escaped: `\\\\\\\\n`)\\n    \"\"\"\\n    if isinstance(multiline_string, (bytes, bytearray)):\\n        multiline_string = multiline_string.decode()\\n\\n    multiline_string = re.sub(\\n        r\\'(\"action_input\"\\\\:\\\\s*\")(.*?)(\")\\',\\n        _replace_new_line,\\n        multiline_string,\\n        flags=re.DOTALL,\\n    )\\n\\n    return multiline_string\\n\\n\\n# Adapted from https://github.com/KillianLucas/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py\\n# MIT License\\n\\n\\n\\n[docs]def parse_partial_json(s: str, *, strict: bool = False) -> Any: \"\"\"Parse a JSON string that may be missing closing braces. Args: s: The JSON string to parse. strict: Whether to use strict parsing. Defaults to False. Returns: The parsed JSON object as a Python dictionary. \"\"\" # Attempt to parse the string as-is. try: return json.loads(s, strict=strict) except json.JSONDecodeError: pass # Initialize variables. new_chars = [] stack = [] is_inside_string = False escaped = False # Process each character in the string one at a time. for char in s: if is_inside_string: if char == \\'\"\\' and not escaped: is_inside_string = False elif char == \"\\\\n\" and not escaped: char = \"\\\\\\\\n\" # Replace the newline character with the escape sequence. elif char == \"\\\\\\\\\": escaped = not escaped else: escaped = False else: if char == \\'\"\\': is_inside_string = True escaped = False elif char == \"{\": stack.append(\"}\") elif char == \"[\": stack.append(\"]\") elif char == \"}\" or char == \"]\": if stack and stack[-1] == char: stack.pop() else: # Mismatched closing character; the input is malformed. return None # Append the processed character to the new string. new_chars.append(char) # If we\\'re still inside a string at the end of processing, # we need to close the string. if is_inside_string: new_chars.append(\\'\"\\') # Reverse the stack to get the closing characters. stack.reverse() # Try to parse mods of string until we succeed or run out of characters. while new_chars: # Close any remaining open structures in the reverse # order that they were opened. # Attempt to parse the modified string as JSON. try: return json.loads(\"\".join(new_chars + stack), strict=strict) except json.JSONDecodeError: # If we still can\\'t parse the string as JSON, # try removing the last character new_chars.pop() # If we got here, we ran out of characters to remove # and still couldn\\'t parse the string as JSON, so return the parse error # for the original string. return json.loads(s, strict=strict)\\n\\n\\n\\n_json_markdown_re = re.compile(r\"```(json)?(.*)\", re.DOTALL)\\n\\n\\n\\n[docs]def parse_json_markdown( json_string: str, *, parser: Callable[[str], Any] = parse_partial_json ) -> dict: \"\"\"Parse a JSON string from a Markdown string. Args: json_string: The Markdown string. Returns: The parsed JSON object as a Python dictionary. \"\"\" try: return _parse_json(json_string, parser=parser) except json.JSONDecodeError: # Try to find JSON string within triple backticks match = _json_markdown_re.search(json_string) # If no match found, assume the entire string is a JSON string if match is None: json_str = json_string else: # If match found, use the content within the backticks json_str = match.group(2) return _parse_json(json_str, parser=parser)\\n\\n\\n\\n_json_strip_chars = \" \\\\n\\\\r\\\\t`\"\\n\\n\\ndef _parse_json(\\n    json_str: str, *, parser: Callable[[str], Any] = parse_partial_json\\n) -> dict:\\n    # Strip whitespace,newlines,backtick from the start and end\\n    json_str = json_str.strip(_json_strip_chars)\\n\\n    # handle newlines and other special characters inside the returned value\\n    json_str = _custom_parser(json_str)\\n\\n    # Parse the JSON string into a Python dictionary\\n    return parser(json_str)'),\n",
       " Document(metadata={'source': 'https://api.python.langchain.com/en/latest/_modules/langchain_core/utils/json.html'}, page_content='_json_strip_chars = \" \\\\n\\\\r\\\\t`\"\\n\\n\\ndef _parse_json(\\n    json_str: str, *, parser: Callable[[str], Any] = parse_partial_json\\n) -> dict:\\n    # Strip whitespace,newlines,backtick from the start and end\\n    json_str = json_str.strip(_json_strip_chars)\\n\\n    # handle newlines and other special characters inside the returned value\\n    json_str = _custom_parser(json_str)\\n\\n    # Parse the JSON string into a Python dictionary\\n    return parser(json_str)\\n\\n\\n\\n[docs]def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict: \"\"\" Parse a JSON string from a Markdown string and check that it contains the expected keys. Args: text: The Markdown string. expected_keys: The expected keys in the JSON string. Returns: The parsed JSON object as a Python dictionary. Raises: OutputParserException: If the JSON string is invalid or does not contain the expected keys. \"\"\" try: json_obj = parse_json_markdown(text) except json.JSONDecodeError as e: raise OutputParserException(f\"Got invalid JSON object. Error: {e}\") from e for key in expected_keys: if key not in json_obj: raise OutputParserException( f\"Got invalid return object. Expected key `{key}` \" f\"to be present, but got {json_obj}\" ) return json_obj\\n\\nÂ© 2023, LangChain, Inc. . Last updated on Dec 09, 2024.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/mongodb/vectorstores/loaders/retrievers/langchain_mongodb.retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.2/api_reference/vectorstores/langchain_community.vectorstores.tair.Tair.html'}, page_content='Page Not Found\\n\\nWe could not find what you were looking for.\\n\\nPlease contact the owner of the site that linked you to the original URL and let them know their link is broken.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/mongodb/chat_message_histories/graphrag/graphrag/graphrag.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/api_reference/mongodb/retrievers/chat_message_histories/utils/retrievers/langchain_mongodb.retrievers.self_querying.MongoDBStructuredQueryTranslator.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "spliter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n",
    "documents = spliter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "# nvidia/nv-embedcode-7b-v1 ---> Mistralbased code optimized\n",
    "# nvidia/llama-3.2-nv-embedqa-1b-v2 ---> Llama based QA optimized\n",
    "embeddings_model = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", \n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    truncate=\"NONE\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectore_store = FAISS.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open on GitHub\\n\\nSetup\\n\\nLangChain documentation consists of two components:\\n\\nMain Documentation: Hosted at python.langchain.com, this comprehensive resource serves as the primary user-facing documentation. It covers a wide array of topics, including tutorials, use cases, integrations, and more, offering extensive guidance on building with LangChain. The content for this documentation lives in the /docs directory of the monorepo.\\n\\nIn-code Documentation: This is documentation of the codebase itself, which is also used to generate the externally facing API Reference. The content for the API reference is autogenerated by scanning the docstrings in the codebase. For this reason we ask that developers document their code well.\\n\\nThe API Reference is largely autogenerated by sphinx from the code and is hosted by Read the Docs.\\n\\nWe appreciate all contributions to the documentation, whether it be fixing a typo, adding a new tutorial or example and whether it be in the main documentation or the API Reference.\\n\\nSimilar to linting, we recognize documentation can be annoying. If you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\\n\\nð Main Documentation\\u200b\\n\\nThe content for the main documentation is located in the /docs directory of the monorepo.\\n\\nThe documentation is written using a combination of ipython notebooks (.ipynb files) and markdown (.mdx files). The notebooks are converted to markdown and then built using Docusaurus 2.\\n\\nFeel free to make contributions to the main documentation! ð¥°\\n\\nAfter modifying the documentation:\\n\\nRun the linting and formatting commands (see below) to ensure that the documentation is well-formatted and free of errors.\\n\\nOptionally build the documentation locally to verify that the changes look good.\\n\\nMake a pull request with the changes.\\n\\nYou can preview and verify that the changes are what you wanted by clicking the View deployment or Visit Preview buttons on the pull request Conversation page. This will take you to a preview of the documentation changes.\\n\\nâï¸ Linting and Building Documentation Locally\\u200b\\n\\nAfter writing up the documentation, you may want to lint and build the documentation locally to ensure that it looks good and is free of errors.\\n\\nIf you\\'re unable to build it locally that\\'s okay as well, as you will be able to see a preview of the documentation on the pull request page.\\n\\nBuilding\\u200b\\n\\nThe code that builds the documentation is located in the /docs directory of the monorepo.\\n\\nIn the following commands, the prefix api_ indicates that those are operations for the API Reference.\\n\\nBefore building the documentation, it is always a good idea to clean the build directory:\\n\\nmake docs_clean\\nmake api_docs_clean\\n\\nNext, you can build the documentation as outlined below:\\n\\nmake docs_build\\nmake api_docs_build\\n\\ntip\\n\\nThe make api_docs_build command takes a long time. If you\\'re making cosmetic changes to the API docs and want to see how they look, use:\\n\\nmake api_docs_quick_preview\\n\\nwhich will just build a small subset of the API reference.\\n\\nFinally, run the link checker to ensure all links are valid:\\n\\nmake docs_linkcheck\\nmake api_docs_linkcheck\\n\\nLinting and Formatting\\u200b\\n\\nThe Main Documentation is linted from the monorepo root. To lint the main documentation, run the following from there:\\n\\nmake lint\\n\\nIf you have formatting-related errors, you can fix them automatically with:\\n\\nmake format\\n\\nâ¨ï¸ In-code Documentation\\u200b\\n\\nThe in-code documentation is largely autogenerated by sphinx from the code and is hosted by Read the Docs.\\n\\nFor the API reference to be useful, the codebase must be well-documented. This means that all functions, classes, and methods should have a docstring that explains what they do, what the arguments are, and what the return value is. This is a good practice in general, but it is especially important for LangChain because the API reference is the primary resource for developers to understand how to use the codebase.\\n\\nWe generally follow the Google Python Style Guide for docstrings.\\n\\nHere is an example of a well-documented function:\\n\\ndef my_function(arg1: int, arg2: str) -> float:\\n    \"\"\"This is a short description of the function. (It should be a single sentence.)\\n\\n    This is a longer description of the function. It should explain what\\n    the function does, what the arguments are, and what the return value is.\\n    It should wrap at 88 characters.\\n\\n    Examples:\\n        This is a section for examples of how to use the function.\\n\\n        .. code-block:: python\\n\\n            my_function(1, \"hello\")\\n\\n    Args:\\n        arg1: This is a description of arg1. We do not need to specify the type since\\n            it is already specified in the function signature.\\n        arg2: This is a description of arg2.\\n\\n    Returns:\\n        This is a description of the return value.\\n    \"\"\"\\n    return 3.14\\n\\nLinting and Formatting\\u200b'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_result = vectore_store.similarity_search(\"What is the purpose of the langchain\")\n",
    "similar_result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the user follwing qestin based on only on the provided contet:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found deepseek-ai/deepseek-r1-distill-llama-8b in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm_client = ChatNVIDIA(\n",
    "    model=\"deepseek-ai/deepseek-r1-distill-llama-8b\",\n",
    "    api_key=os.getenv('NVIDIA_API_KEY'), \n",
    "    temperature=0.6,\n",
    "    top_p=0.7,\n",
    "    max_tokens=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the user follwing qestin based on only on the provided contet:\\n\\n    {context}\\n\\n\\n    '), additional_kwargs={})])\n",
       "| ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='deepseek-ai/deepseek-r1-distill-llama-8b', temperature=0.6, max_tokens=4096, top_p=0.7)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain= create_stuff_documents_chain(\n",
    "llm_client, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'NVIDIAEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x79e678d29820>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n    Answer the user follwing qestin based on only on the provided contet:\\n\\n    {context}\\n\\n\\n    '), additional_kwargs={})])\n",
       "            | ChatNVIDIA(base_url='https://integrate.api.nvidia.com/v1', model='deepseek-ai/deepseek-r1-distill-llama-8b', temperature=0.6, max_tokens=4096, top_p=0.7)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "retriver = vectore_store.as_retriever()\n",
    "retrival_chain = create_retrieval_chain(retriver, document_chain)\n",
    "retrival_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'give me a list of All chat models available',\n",
       " 'context': [Document(id='c96521cf-da43-4313-bbc3-1b9355323178', metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='Source code for langchain_community.llms.databricks\\n\\nimport os\\nimport re\\nimport warnings\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, Callable, Dict, List, Mapping, Optional\\n\\nimport requests\\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\\nfrom langchain_core.language_models import LLM\\nfrom langchain_core.pydantic_v1 import (\\n    BaseModel,\\n    Field,\\n    PrivateAttr,\\n    root_validator,\\n    validator,\\n)\\n\\n__all__ = [\"Databricks\"]\\n\\n\\nclass _DatabricksClientBase(BaseModel, ABC):\\n    \"\"\"A base JSON API client that talks to Databricks.\"\"\"\\n\\n    api_url: str\\n    api_token: str\\n\\n    def request(self, method: str, url: str, request: Any) -> Any:\\n        headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\\n        response = requests.request(\\n            method=method, url=url, headers=headers, json=request\\n        )\\n        # TODO: error handling and automatic retries\\n        if not response.ok:\\n            raise ValueError(f\"HTTP {response.status_code} error: {response.text}\")\\n        return response.json()\\n\\n    def _get(self, url: str) -> Any:\\n        return self.request(\"GET\", url, None)\\n\\n    def _post(self, url: str, request: Any) -> Any:\\n        return self.request(\"POST\", url, request)\\n\\n    @abstractmethod\\n    def post(\\n        self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None\\n    ) -> Any: ...\\n\\n    @property\\n    def llm(self) -> bool:\\n        return False\\n\\n\\ndef _transform_completions(response: Dict[str, Any]) -> str:\\n    return response[\"choices\"][0][\"text\"]\\n\\n\\ndef _transform_llama2_chat(response: Dict[str, Any]) -> str:\\n    return response[\"candidates\"][0][\"text\"]\\n\\n\\ndef _transform_chat(response: Dict[str, Any]) -> str:\\n    return response[\"choices\"][0][\"message\"][\"content\"]\\n\\n\\nclass _DatabricksServingEndpointClient(_DatabricksClientBase):\\n    \"\"\"An API client that talks to a Databricks serving endpoint.\"\"\"\\n\\n    host: str\\n    endpoint_name: str\\n    databricks_uri: str\\n    client: Any = None\\n    external_or_foundation: bool = False\\n    task: Optional[str] = None\\n\\n    def __init__(self, **data: Any):\\n        super().__init__(**data)\\n\\n        try:\\n            from mlflow.deployments import get_deploy_client\\n\\n            self.client = get_deploy_client(self.databricks_uri)\\n        except ImportError as e:\\n            raise ImportError(\\n                \"Failed to create the client. \"\\n                \"Please install mlflow with `pip install mlflow`.\"\\n            ) from e\\n\\n        endpoint = self.client.get_endpoint(self.endpoint_name)\\n        self.external_or_foundation = endpoint.get(\"endpoint_type\", \"\").lower() in (\\n            \"external_model\",\\n            \"foundation_model_api\",\\n        )\\n        if self.task is None:\\n            self.task = endpoint.get(\"task\")\\n\\n    @property\\n    def llm(self) -> bool:\\n        return self.task in (\"llm/v1/chat\", \"llm/v1/completions\", \"llama2/chat\")\\n\\n    @root_validator(pre=True)\\n    def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n        if \"api_url\" not in values:\\n            host = values[\"host\"]\\n            endpoint_name = values[\"endpoint_name\"]\\n            api_url = f\"https://{host}/serving-endpoints/{endpoint_name}/invocations\"\\n            values[\"api_url\"] = api_url\\n        return values\\n\\n    def post(\\n        self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None\\n    ) -> Any:\\n        if self.external_or_foundation:\\n            resp = self.client.predict(endpoint=self.endpoint_name, inputs=request)\\n            if transform_output_fn:\\n                return transform_output_fn(resp)\\n\\n            if self.task == \"llm/v1/chat\":\\n                return _transform_chat(resp)\\n            elif self.task == \"llm/v1/completions\":\\n                return _transform_completions(resp)\\n\\n            return resp\\n        else:\\n            # See https://docs.databricks.com/machine-learning/model-serving/score-model-serving-endpoints.html\\n            wrapped_request = {\"dataframe_records\": [request]}\\n            response = self.client.predict(\\n                endpoint=self.endpoint_name, inputs=wrapped_request\\n            )\\n            preds = response[\"predictions\"]\\n            # For a single-record query, the result is not a list.\\n            pred = preds[0] if isinstance(preds, list) else preds\\n            if self.task == \"llama2/chat\":\\n                return _transform_llama2_chat(pred)\\n            return transform_output_fn(pred) if transform_output_fn else pred\\n\\n\\nclass _DatabricksClusterDriverProxyClient(_DatabricksClientBase):\\n    \"\"\"An API client that talks to a Databricks cluster driver proxy app.\"\"\"\\n\\n    host: str\\n    cluster_id: str\\n    cluster_driver_port: str'),\n",
       "  Document(id='d38e172f-471b-4cb5-a939-e42646a613b1', metadata={'source': 'https://python.langchain.com/v0.2/api_reference/_modules/langchain_community/llms/databricks.html'}, page_content='[docs]class Databricks(LLM): \"\"\"Databricks serving endpoint or a cluster driver proxy app for LLM. It supports two endpoint types: * **Serving endpoint** (recommended for both production and development). We assume that an LLM was deployed to a serving endpoint. To wrap it as an LLM you must have \"Can Query\" permission to the endpoint. Set ``endpoint_name`` accordingly and do not set ``cluster_id`` and ``cluster_driver_port``. If the underlying model is a model registered by MLflow, the expected model signature is: * inputs:: [{\"name\": \"prompt\", \"type\": \"string\"}, {\"name\": \"stop\", \"type\": \"list[string]\"}] * outputs: ``[{\"type\": \"string\"}]`` If the underlying model is an external or foundation model, the response from the endpoint is automatically transformed to the expected format unless ``transform_output_fn`` is provided. * **Cluster driver proxy app** (recommended for interactive development). One can load an LLM on a Databricks interactive cluster and start a local HTTP server on the driver node to serve the model at ``/`` using HTTP POST method with JSON input/output. Please use a port number between ``[3000, 8000]`` and let the server listen to the driver IP address or simply ``0.0.0.0`` instead of localhost only. To wrap it as an LLM you must have \"Can Attach To\" permission to the cluster. Set ``cluster_id`` and ``cluster_driver_port`` and do not set ``endpoint_name``. The expected server schema (using JSON schema) is: * inputs:: {\"type\": \"object\", \"properties\": { \"prompt\": {\"type\": \"string\"}, \"stop\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"prompt\"]}` * outputs: ``{\"type\": \"string\"}`` If the endpoint model signature is different or you want to set extra params, you can use `transform_input_fn` and `transform_output_fn` to apply necessary transformations before and after the query. \"\"\" host: str = Field(default_factory=get_default_host) \"\"\"Databricks workspace hostname. If not provided, the default value is determined by * the ``DATABRICKS_HOST`` environment variable if present, or * the hostname of the current Databricks workspace if running inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode. \"\"\" api_token: str = Field(default_factory=get_default_api_token) \"\"\"Databricks personal access token. If not provided, the default value is determined by * the ``DATABRICKS_TOKEN`` environment variable if present, or * an automatically generated temporary token if running inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode. \"\"\" endpoint_name: Optional[str] = None \"\"\"Name of the model serving endpoint. You must specify the endpoint name to connect to a model serving endpoint. You must not set both ``endpoint_name`` and ``cluster_id``. \"\"\" cluster_id: Optional[str] = None \"\"\"ID of the cluster if connecting to a cluster driver proxy app. If neither ``endpoint_name`` nor ``cluster_id`` is not provided and the code runs inside a Databricks notebook attached to an interactive cluster in \"single user\" or \"no isolation shared\" mode, the current cluster ID is used as default. You must not set both ``endpoint_name`` and ``cluster_id``. \"\"\" cluster_driver_port: Optional[str] = None \"\"\"The port number used by the HTTP server running on the cluster driver node. The server should listen on the driver IP address or simply ``0.0.0.0`` to connect. We recommend the server using a port number between ``[3000, 8000]``. \"\"\" model_kwargs: Optional[Dict[str, Any]] = None \"\"\" Deprecated. Please use ``extra_params`` instead. Extra parameters to pass to the endpoint. \"\"\" transform_input_fn: Optional[Callable] = None \"\"\"A function that transforms ``{prompt, stop, **kwargs}`` into a JSON-compatible request object that the endpoint accepts. For example, you can apply a prompt template to the input prompt. \"\"\" transform_output_fn: Optional[Callable[..., str]] = None \"\"\"A function that transforms the output from the endpoint to the generated text. \"\"\" databricks_uri: str = \"databricks\" \"\"\"The databricks URI. Only used when using a serving endpoint.\"\"\" temperature: float = 0.0 \"\"\"The sampling temperature.\"\"\" n: int = 1 \"\"\"The number of completion choices to generate.\"\"\" stop: Optional[List[str]] = None \"\"\"The stop sequence.\"\"\" max_tokens: Optional[int] = None \"\"\"The maximum number of tokens to generate.\"\"\" extra_params: Dict[str, Any] = Field(default_factory=dict) \"\"\"Any extra parameters to pass to the endpoint.\"\"\" task: Optional[str] = None \"\"\"The task of the endpoint. Only used when using a serving endpoint. If not provided, the task is automatically inferred from the endpoint. \"\"\" allow_dangerous_deserialization: bool = False \"\"\"Whether to allow dangerous deserialization of the data which involves loading data using pickle. If the data has been modified by a malicious actor, it can deliver a malicious payload that results in execution of arbitrary code on the target machine. \"\"\" _client:'),\n",
       "  Document(id='1161485a-ce16-4e94-9897-0452f8f8758e', metadata={'source': 'https://python.langchain.com/api_reference/mongodb/retrievers/pipelines/pipelines/retrievers/langchain_mongodb.retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\"),\n",
       "  Document(id='9fe11d3b-b580-42ce-835d-cd906d6a0fc3', metadata={'source': 'https://python.langchain.com/api_reference/mongodb/chat_message_histories/loaders/loaders/utils/langchain_mongodb.utils.cosine_similarity.html'}, page_content=\"langchain-mongodb: 0.6.0#\\n\\nIntegrate your operational database and vector search in a single, unified, fully managed platform with full vector database capabilities on MongoDB Atlas.\\n\\nStore your operational data, metadata, and vector embeddings in oue VectorStore, MongoDBAtlasVectorSearch. Insert into a Chain via a Vector, FullText, or Hybrid Retriever.\\n\\nagent_toolkit#\\n\\nClasses\\n\\nagent_toolkit.database.MongoDBDatabase (...) Wrapper around a MongoDB database. agent_toolkit.tool.BaseMongoDBDatabaseTool Base tool for interacting with a MongoDB database. agent_toolkit.tool.InfoMongoDBDatabaseTool Tool for getting metadata about a MongoDB database. agent_toolkit.tool.ListMongoDBDatabaseTool Tool for getting collection names. agent_toolkit.tool.QueryMongoDBCheckerTool Use an LLM to check if a query is correct. agent_toolkit.tool.QueryMongoDBDatabaseTool Tool for querying a MongoDB database. agent_toolkit.toolkit.MongoDBDatabaseToolkit MongoDBDatabaseToolkit for interacting with MongoDB databases.\\n\\ncache#\\n\\nClasses\\n\\ncache.MongoDBAtlasSemanticCache (...[,\\xa0...]) MongoDB Atlas Semantic cache. cache.MongoDBCache (connection_string[,\\xa0...]) MongoDB Atlas cache\\n\\nchat_message_histories#\\n\\nClasses\\n\\nchat_message_histories.MongoDBChatMessageHistory (...) Chat message history that stores history in MongoDB.\\n\\ndocstores#\\n\\nClasses\\n\\ndocstores.MongoDBDocStore (collection[,\\xa0text_key]) MongoDB Collection providing BaseStore interface.\\n\\ngraphrag#\\n\\nClasses\\n\\ngraphrag.graph.MongoDBGraphStore (*[,\\xa0...]) GraphRAG DataStore\\n\\nindex#\\n\\nFunctions\\n\\nindex.create_fulltext_search_index (...[,\\xa0...]) Experimental Utility function to create an Atlas Search index index.create_vector_search_index (collection,\\xa0...) Experimental Utility function to create a vector search index index.drop_vector_search_index (collection,\\xa0...) Drop a created vector search index index.update_vector_search_index (collection,\\xa0...) Update a search index.\\n\\nindexes#\\n\\nClasses\\n\\nindexes.MongoDBRecordManager (collection) A MongoDB-based implementation of the record manager.\\n\\nloaders#\\n\\nClasses\\n\\nloaders.MongoDBLoader (collection,\\xa0*[,\\xa0...]) Document Loaders are classes to load Documents.\\n\\npipelines#\\n\\nFunctions\\n\\npipelines.combine_pipelines (pipeline,\\xa0stage,\\xa0...) Combines two aggregations into a single result set in-place. pipelines.final_hybrid_stage (scores_fields,\\xa0...) Sum weighted scores, sort, and apply limit. pipelines.reciprocal_rank_stage (score_field) Stage adds Reciprocal Rank Fusion weighting. pipelines.text_search_stage (query,\\xa0...[,\\xa0...]) Full-Text search using Lucene's standard (BM25) analyzer pipelines.vector_search_stage (query_vector,\\xa0...) Vector Search Stage without Scores.\\n\\nretrievers#\\n\\nClasses\\n\\nretrievers.full_text_search.MongoDBAtlasFullTextSearchRetriever Retriever performs full-text searches using Lucene's standard (BM25) analyzer. retrievers.graphrag.MongoDBGraphRAGRetriever RunnableSerializable API of MongoDB GraphRAG. retrievers.hybrid_search.MongoDBAtlasHybridSearchRetriever Hybrid Search Retriever combines vector and full-text searches weighting them the via Reciprocal Rank Fusion (RRF) algorithm. retrievers.parent_document.MongoDBAtlasParentDocumentRetriever MongoDB Atlas's ParentDocumentRetriever retrievers.self_querying.MongoDBAtlasSelfQueryRetriever Retriever that uses an LLM to deduce filters for Vector Search algorithm. retrievers.self_querying.MongoDBStructuredQueryTranslator () Translator between MongoDB Query API and LangChain's StructuredQuery.\\n\\nutils#\\n\\nFunctions\\n\\nutils.cosine_similarity (X,\\xa0Y) Row-wise cosine similarity between two equal-width matrices. utils.make_serializable (obj) Recursively cast values in a dict to a form able to json.dump utils.maximal_marginal_relevance (...[,\\xa0...]) Compute Maximal Marginal Relevance (MMR). utils.oid_to_str (oid) Convert MongoDB's internal BSON ObjectId into a simple str for compatibility. utils.str_to_oid (str_repr) Attempt to cast string representation of id to MongoDB's internal BSON ObjectId.\\n\\nvectorstores#\\n\\nClasses\\n\\nvectorstores.MongoDBAtlasVectorSearch (...[,\\xa0...]) MongoDB Atlas vector store integration.\")],\n",
       " 'answer': '<think>\\nOkay, so I\\'m trying to figure out how to use the LangChain MongoDB integration. I\\'ve seen the code snippets provided, but I\\'m not entirely sure how to start. Let me break it down step by step.\\n\\nFirst, I notice that there are several classes related to MongoDB, like MongoDBDatabase, MongoDBDocStore, and others. I think these are meant to interact with MongoDB databases and collections. The user mentioned integrating operational data and vector search, so I guess these classes help with that.\\n\\nLooking at the code, I see that there\\'s a MongoDBDatabase class under agent_toolkit.database. It probably wraps a MongoDB instance, allowing for operations like querying. Then there\\'s MongoDBDocStore, which seems to be a BaseStore implementation, meaning it\\'s a generic document store. It uses a collection and a text key, which I assume is for full-text search.\\n\\nThe user also mentioned using a vector store, so I check the vectorstores module. There\\'s MongoDBAtlasVectorSearch, which integrates with MongoDB Atlas. This must handle vector embeddings and searches, which is crucial for tasks like retrieval.\\n\\nI\\'m a bit confused about how to start using these classes. Do I need to create an instance of MongoDBDatabase first? Probably, using the connection string. Then, from that, I can get a docstore or vectorstore.\\n\\nWait, the user mentioned using a chain via a Vector, FullText, or Hybrid Retriever. So maybe I should use one of these retrievers. The code shows classes like MongoDBAtlasFullTextSearchRetriever and MongoDBGraphRAGRetriever. I think the FullTextSearchRetriever uses the docstore for full-text searches, while the GraphRAGRetriever uses a graph structure for more advanced queries.\\n\\nI\\'m not sure how to structure the chain. Do I start with a text input, then use the retriever to find relevant documents, and then maybe use an LLM to generate a response? That makes sense. So the flow would be: user input â retriever â LLM.\\n\\nLet me outline the steps I think I need to take:\\n\\n1. Initialize the MongoDBDatabase with a connection string.\\n2. Create a docstore from the database using a specific collection and text key.\\n3. Choose a retriever, either FullTextSearchRetriever or GraphRAGRetriever, depending on the use case.\\n4. Use the retriever to search for relevant documents based on the user\\'s input.\\n5. Pass the retrieved documents to an LLM to generate a response.\\n6. Return the generated text as the output.\\n\\nI\\'m not entirely sure about the connection string format. It might be something like \"mongodb://username:password@host:port/database\". Also, I need to make sure the collection exists in MongoDB before using it.\\n\\nAnother thing I\\'m unsure about is how the transform_input_fn and transform_output_fn work. Do I need to provide custom functions to modify the input or output? The code mentions they can be used for transformations, so maybe they\\'re optional unless I need specific formatting.\\n\\nI should also consider error handling. What if the connection to MongoDB fails? Are there any exceptions being raised that I need to catch? The code doesn\\'t show much error handling, so I might need to add that.\\n\\nFor the vectorstore, I think it\\'s used when I have vector embeddings. So if I have embeddings stored in MongoDB, I can use the vectorstore to perform similarity searches. But I\\'m not sure how to integrate that with the rest of the chain. Maybe after retrieving documents with a retriever, I can use the vectorstore to find similar embeddings.\\n\\nWait, the user mentioned using a HybridRetriever, which combines vector and full-text searches. So perhaps I can use that to get the best of both worlds: find relevant documents using full-text and then use vector similarity for ranking.\\n\\nI\\'m also a bit confused about the difference between using a serving endpoint and a cluster driver proxy in Databricks. The Databricks class seems to handle both, but I\\'m not sure when to use which. Maybe serving endpoints are for production, while the cluster driver is for development.\\n\\nI think I need to start by setting up the MongoDB connection. Let me try writing some code:\\n\\n```python\\nfrom langchain_community.llms.databricks import Databricks\\nfrom langchain_community.documentstores.mongodb import MongoDBDocStore\\nfrom langchain_community.retrievers.mongodb import MongoDBAtlasFullTextSearchRetriever\\n\\n# Initialize MongoDB\\ndb = MongoDBDatabase(mongo_uri=\"mongodb://localhost:27017/mydb\")\\n\\n# Create docstore\\ndocstore = MongoDBDocStore(db, collection=\"my_collection\", text_key=\"text_field\")\\n\\n# Create retriever\\nretriever = MongoDBAtlasFullTextSearchRetriever(docstore)\\n\\n# Use the retriever with an LLM\\nllm = Databricks()\\nresponse = llm([input_text], transform_input_fn=retriever, transform_output_fn=llm)\\n```\\n\\nWait, that doesn\\'t look right. The Databricks class might need more parameters, like the API token. Also, I\\'m not sure if the transform_input_fn should be the retriever. Maybe I need to structure it differently.\\n\\nPerhaps:\\n\\n```python\\n# Prepare the document store\\ndocstore = MongoDBDocStore(db, collection=\"my_collection\", text_key=\"text_key\")\\n\\n# Prepare the vector store if needed\\nvectorstore = MongoDBAtlasVectorSearch(db, collection=\"vector_collection\")\\n\\n# Create a hybrid retriever\\nretriever = MongoDBAtlasHybridSearchRetriever(docstore, vectorstore)\\n\\n# Use the retriever with the LLM\\nresponse = llm([user_input], transform_input_fn=retriever, transform_output_fn=llm)\\n```\\n\\nI\\'m still not sure if I\\'m using the vectorstore correctly. Maybe the HybridRetriever combines both full-text and vector searches, so I can get documents and their vector similarities.\\n\\nI think I need to look into the documentation or examples to see how these classes are typically used. Maybe there are specific methods or parameters I\\'m missing.\\n\\nAnother thing is the temperature and sampling settings. The Databricks class has parameters like temperature and n, which affect the LLM\\'s sampling. I should make sure to set those appropriately for the desired output.\\n\\nI\\'m also wondering about the performance. Since this is integrating with MongoDB, I need to ensure that the queries are efficient. Maybe using indexes or optimizing the document structure can help.\\n\\nIn summary, my plan is:\\n\\n1. Set up MongoDB connection using MongoDBDatabase.\\n2. Create a docstore and vectorstore if needed.\\n3. Choose the appropriate retriever (FullText, GraphRAG, or Hybrid).\\n4. Use the retriever within a LangChain pipeline to fetch relevant documents.\\n5. Pass the results to an LLM for generation.\\n6. Handle any necessary transformations and output formatting.\\n\\nI think I need to experiment with these steps, maybe starting with a simple setup and gradually adding more complex retrievers and transformations as needed.\\n</think>\\n\\nTo use the LangChain MongoDB integration, follow these steps:\\n\\n1. **Set Up MongoDB Connection**:\\n   - Use the `MongoDBDatabase` class to connect to your MongoDB instance.\\n   - Provide a valid MongoDB connection string, e.g., `mongodb://localhost:27017/mydb`.\\n\\n2. **Create a Document Store**:\\n   - Use `MongoDBDocStore` to create a document store linked to a specific collection and field for full-text search.\\n   - Example:\\n     ```python\\n     docstore = MongoDBDocStore(db, collection=\"my_collection\", text_key=\"text_field\")\\n     ```\\n\\n3. **Choose a Retriever**:\\n   - **Full-Text Search**: Use `MongoDBAtlasFullTextSearchRetriever` for basic full-text searches.\\n   - **Graph RAG**: Use `MongoDBGraphRAGRetriever` for advanced queries leveraging document relationships.\\n   - **Hybrid Search**: Combine both with `MongoDBAtlasHybridSearchRetriever` for enhanced retrieval.\\n\\n4. **Use the Retriever with an LLM**:\\n   - Pass the retriever as a transformation function to the LLM.\\n   - Example:\\n     ```python\\n     llm = Databricks(api_token=\"your_api_token\")\\n     response = llm([user_input], transform_input_fn=retriever, transform_output_fn=llm)\\n     ```\\n\\n5. **Optional: Use Vector Store**:\\n   - If using vector embeddings, create a `MongoDBAtlasVectorSearch` instance.\\n   - Combine with a hybrid retriever for enhanced search capabilities.\\n\\n6. **Handle Transformations and Output**:\\n   - Use `transform_input_fn` to modify input for the LLM.\\n   - Use `transform_output_fn` to format the LLM\\'s response as needed.\\n\\n7. **Example Pipeline**:\\n   ```python\\n   # Initialize MongoDB\\n   db = MongoDBDatabase(mongo_uri=\"mongodb://localhost:27017/mydb\")\\n\\n   # Create document store and vector store\\n   docstore = MongoDBDocStore(db, collection=\"my_collection\", text_key=\"text_field\")\\n   vectorstore = MongoDBAtlasVectorSearch(db, collection=\"vector_collection\")\\n\\n   # Create hybrid retriever\\n   retriever = MongoDBAtlasHybridSearchRetriever(docstore, vectorstore)\\n\\n   # Use with LLM\\n   llm = Databricks(api_token=\"your_api_token\", temperature=0.7, n=1)\\n   response = llm([user_input], transform_input_fn=retriever, transform_output_fn=llm)\\n   ```\\n\\n8. **Considerations**:\\n   - Ensure your MongoDB collection exists and is properly indexed.\\n   - Handle potential connection errors and exceptions.\\n   - Explore different retrievers based on your application\\'s needs.\\n   - Adjust parameters like `temperature` and `n` for desired output quality and diversity.\\n\\nBy following these steps, you can effectively integrate MongoDB with LangChain for enhanced document retrieval and LLM-based responses.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_chain.invoke({\"input\":\"give me a list of All chat models available\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to contribute to the LangChain documentation based on the provided content. Let me start by reading through the setup and the documentation sections to understand what's expected.\n",
      "\n",
      "From the setup, I see that LangChain has both main documentation and in-code documentation. The main documentation is on python.langchain.com and is in the /docs directory. It's written using ipynb and mdx files and built with Docusaurus 2. The in-code documentation is auto-generated from the code using sphinx and is hosted on Read the Docs.\n",
      "\n",
      "The user wants me to answer a question based on the provided content. Since the user didn't specify a question, I'll assume they might be asking about the process of contributing to the documentation or how to write effective docstrings.\n",
      "\n",
      "Looking at the in-code documentation section, it's crucial for the API reference. The user emphasizes that all functions, classes, and methods should have well-documented docstrings following the Google Python Style Guide. An example is provided, which includes a function definition with a docstring that explains the function's purpose, arguments, and return value, along with examples and sections for Args, Returns, etc.\n",
      "\n",
      "So, if someone is asking how to contribute to the in-code documentation, I should explain the importance of docstrings, how to write them properly, and maybe provide an example. Alternatively, if they're asking about the main documentation, I can talk about writing tutorials or use cases in the /docs directory.\n",
      "\n",
      "I should also mention the linting and building process for the main documentation, as well as the structure of the codebase, which includes classes and functions related to MongoDB integration, such as MongoDBDatabase, loaders, retrievers, and vectorstores.\n",
      "\n",
      "In summary, my answer should cover how to contribute to either the main documentation or the in-code API reference, emphasizing the use of docstrings and the structure of the LangChain codebase.\n",
      "</think>\n",
      "\n",
      "To contribute to the LangChain documentation, you can focus on either the main documentation or the in-code API reference. Here's a structured approach:\n",
      "\n",
      "### Main Documentation\n",
      "1. **Location**: Found in the `/docs` directory of the monorepo.\n",
      "2. **Content Types**: Uses ipynb (Jupyter notebooks) and mdx (Markdown) files.\n",
      "3. **Tools**: Built using Docusaurus 2.\n",
      "4. **Contributing**:\n",
      "   - Write tutorials, use cases, and integration guides.\n",
      "   - Edit existing content or create new files.\n",
      "5. **Process**:\n",
      "   - Modify the documentation.\n",
      "   - Run linting and formatting commands to ensure quality.\n",
      "   - Optionally build locally for preview.\n",
      "   - Create a pull request with your changes.\n",
      "\n",
      "### In-Code Documentation\n",
      "1. **Auto-Generation**: Uses sphinx to create the API reference from docstrings.\n",
      "2. **Importance**: Essential for developers to understand the codebase.\n",
      "3. **Guidelines**: Follow the Google Python Style Guide for clarity and consistency.\n",
      "4. **Example**:\n",
      "   ```python\n",
      "   def my_function(arg1: int, arg2: str) -> float:\n",
      "       \"\"\"Short description here.\n",
      "\n",
      "       Longer explanation here. Includes examples and detailed sections.\n",
      "       \"\"\"\n",
      "       return 3.14\n",
      "   ```\n",
      "   - Include purpose, arguments, return value, examples, and sections like Args, Returns.\n",
      "\n",
      "### Contributing to MongoDB Integration\n",
      "The LangChain codebase includes components like `MongoDBDatabase`, `MongoDBLoader`, `MongoDBRetriever`, and `MongoDBVectorStore`. When documenting these, ensure:\n",
      "- Class purposes are clear.\n",
      "- Function arguments and returns are well-explained.\n",
      "- Examples illustrate usage.\n",
      "\n",
      "### Linting and Building\n",
      "- For main docs: Run `make lint` and `make format` from the root.\n",
      "- For API reference: Use `make api_docs_build` or `make api_docs_quick_preview`.\n",
      "\n",
      "By following these steps, you can effectively contribute to the LangChain documentation, whether through comprehensive main docs or detailed in-code references."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for chunk in retrival_chain.stream({\n",
    "        \"input\": \"What is the langchain project all about?\"\n",
    "    }):\n",
    "        if 'answer' in chunk:\n",
    "            print(chunk['answer'], end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
